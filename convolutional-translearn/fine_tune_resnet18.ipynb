{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from fine_tuning_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.0_4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## If want to keep a track of your network on tensorboard, set USE_TENSORBOARD TO 1 in config file.\n",
    "\n",
    "if USE_TENSORBOARD:\n",
    "    from pycrayon import CrayonClient\n",
    "    cc = CrayonClient(hostname=TENSORBOARD_SERVER)\n",
    "    try:\n",
    "        cc.remove_experiment(EXP_NAME)\n",
    "    except:\n",
    "        pass\n",
    "    foo = cc.create_experiment(EXP_NAME)\n",
    "\n",
    "\n",
    "## If want to use the GPU, set GPU_MODE TO 1 in config file\n",
    "\n",
    "use_gpu = GPU_MODE\n",
    "if use_gpu:\n",
    "    torch.cuda.set_device(CUDA_DEVICE)\n",
    "\n",
    "count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### SECTION 1 - data loading and shuffling/augmentation/normalization : all handled by torch automatically.\n",
    "\n",
    "# use imagenet dataset's mean and standard deviation to normalize their dataset approximately. These numbers are imagenet mean and standard deviation!\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = DATA_DIR\n",
    "dsets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "         for x in ['train', 'val']}\n",
    "dset_loaders = {x: torch.utils.data.DataLoader(dsets[x], batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True, num_workers=25)\n",
    "                for x in ['train', 'val']}\n",
    "dset_sizes = {x: len(dsets[x]) for x in ['train', 'val']}\n",
    "dset_classes = dsets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### SECTION 2 : Writing the functions that do training and validation phase. \n",
    "\n",
    "def train_model(model, criterion, optimizer, lr_scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model = model\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                mode='train'\n",
    "                optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()\n",
    "                mode='val'\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            counter=0\n",
    "            # Iterate over data.\n",
    "            for data in dset_loaders[phase]:\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    try:\n",
    "                        inputs, labels = Variable(inputs.float().cuda()),                             \n",
    "                        Variable(labels.long().cuda())\n",
    "                    except:\n",
    "                        print(inputs,labels)\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # Set gradient to zero to delete history of computations in previous epoch. Track operations so that differentiation can be done automatically.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "#                 print('loss done')                \n",
    "                # Just so that you can keep track that something's happening and don't feel like the program isn't running.\n",
    "                if counter%50==0:\n",
    "                    print(\"Reached iteration \",counter)\n",
    "                counter+=1\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "#                     print('loss backward')\n",
    "                    loss.backward()\n",
    "#                     print('done loss backward')\n",
    "                    optimizer.step()\n",
    "#                     print('done optim')\n",
    "                # print evaluation statistics\n",
    "                try:\n",
    "                    running_loss += loss.data[0]\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                except:\n",
    "                    print('unexpected error, could not calculate loss or do a sum.')\n",
    "            print('trying epoch loss')\n",
    "            epoch_loss = running_loss / dset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dset_sizes[phase]\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                if USE_TENSORBOARD:\n",
    "                    foo.add_scalar_value('epoch_loss',epoch_loss,step=epoch)\n",
    "                    foo.add_scalar_value('epoch_acc',epoch_acc,step=epoch)\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    print('new best accuracy = ',best_acc)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('returning and looping back')\n",
    "    return best_model\n",
    "\n",
    "# This function changes the learning rate over the training model.\n",
    "def exp_lr_scheduler(optimizer, epoch, init_lr=BASE_LR, lr_decay_epoch=EPOCH_DECAY):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "    lr = init_lr * (DECAY_WEIGHT**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        print('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /Users/Viola/.torch/models/resnet18-5c106cde.pth\n",
      "100%|██████████| 46827520/46827520 [00:05<00:00, 7886418.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "LR is set to 0.001\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "trying epoch loss\n",
      "train Loss: 0.1185 Acc: 0.5410\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "trying epoch loss\n",
      "val Loss: 0.1511 Acc: 0.4575\n",
      "new best accuracy =  0.45751633986928103\n",
      "Epoch 1/99\n",
      "----------\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "trying epoch loss\n",
      "train Loss: 0.0790 Acc: 0.5205\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "trying epoch loss\n",
      "val Loss: 0.0843 Acc: 0.4314\n",
      "Epoch 2/99\n",
      "----------\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "trying epoch loss\n",
      "train Loss: 0.0773 Acc: 0.5164\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "trying epoch loss\n",
      "val Loss: 0.1038 Acc: 0.4902\n",
      "new best accuracy =  0.49019607843137253\n",
      "Epoch 3/99\n",
      "----------\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "trying epoch loss\n",
      "train Loss: 0.0691 Acc: 0.6926\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "trying epoch loss\n",
      "val Loss: 0.0770 Acc: 0.5359\n",
      "new best accuracy =  0.5359477124183006\n",
      "Epoch 4/99\n",
      "----------\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "trying epoch loss\n",
      "train Loss: 0.0728 Acc: 0.5615\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "trying epoch loss\n",
      "val Loss: 0.0967 Acc: 0.4641\n",
      "Epoch 5/99\n",
      "----------\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "trying epoch loss\n",
      "train Loss: 0.0692 Acc: 0.6107\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "trying epoch loss\n",
      "val Loss: 0.0799 Acc: 0.5882\n",
      "new best accuracy =  0.5882352941176471\n",
      "Epoch 6/99\n",
      "----------\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "trying epoch loss\n",
      "train Loss: 0.0710 Acc: 0.5574\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "trying epoch loss\n",
      "val Loss: 0.0888 Acc: 0.5948\n",
      "new best accuracy =  0.5947712418300654\n",
      "Epoch 7/99\n",
      "----------\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "trying epoch loss\n",
      "train Loss: 0.0711 Acc: 0.5738\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "trying epoch loss\n",
      "val Loss: 0.0766 Acc: 0.6209\n",
      "new best accuracy =  0.6209150326797386\n",
      "Epoch 8/99\n",
      "----------\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "trying epoch loss\n",
      "train Loss: 0.0681 Acc: 0.5902\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "trying epoch loss\n",
      "val Loss: 0.0842 Acc: 0.5686\n",
      "Epoch 9/99\n",
      "----------\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "trying epoch loss\n",
      "train Loss: 0.0708 Acc: 0.5697\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "loss done\n",
      "trying epoch loss\n",
      "val Loss: 0.0892 Acc: 0.5817\n",
      "Epoch 10/99\n",
      "----------\n",
      "loss done\n",
      "Reached iteration  0\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n",
      "done loss backward\n",
      "done optim\n",
      "loss done\n",
      "loss backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-517:\n",
      "Process Process-520:\n",
      "Process Process-523:\n",
      "Process Process-518:\n",
      "Process Process-516:\n",
      "Process Process-519:\n",
      "Process Process-512:\n",
      "Process Process-513:\n",
      "Process Process-511:\n",
      "Process Process-510:\n",
      "Process Process-514:\n",
      "Process Process-509:\n",
      "Process Process-508:\n",
      "Process Process-507:\n",
      "Process Process-505:\n",
      "Process Process-504:\n",
      "Process Process-503:\n",
      "Process Process-502:\n",
      "Process Process-522:\n",
      "Process Process-524:\n",
      "Process Process-506:\n",
      "Process Process-521:\n",
      "Process Process-501:\n",
      "Process Process-515:\n",
      "Process Process-525:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/Viola/anaconda/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cd270206a1a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Run the functions and save the best model in the function model_ft.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m---> 23\u001b[0;31m                        num_epochs=100)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d154685b232a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, lr_scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done loss backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Viola/anaconda/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Viola/anaconda/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### SECTION 3 : DEFINING MODEL ARCHITECTURE.\n",
    "\n",
    "# use Resnet18\n",
    "# Set the number of classes in the config file by setting the right value for NUM_CLASSES.\n",
    "\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_gpu:\n",
    "    criterion.cuda()\n",
    "    model_ft.cuda()\n",
    "\n",
    "optimizer_ft = optim.RMSprop(model_ft.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "# Run the functions and save the best model in the function model_ft.\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=1)\n",
    "\n",
    "# Save model\n",
    "model_ft.save_state_dict('fine_tuned_best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
